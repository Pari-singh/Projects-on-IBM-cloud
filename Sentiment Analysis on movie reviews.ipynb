{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "## IMDB review sentiment analysis\n25000 movie reviews from IMDB, labelled good or bad\nData is available in the Keras dataset and is processed as sequence of integers, i.e., we aren't directly dealing with word vocab.\nWe will embed the sentences with embedding layer and then learn the sequential structure with LSTM", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "from keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.datasets import imdb", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 2
        }, 
        {
            "source": "max_feature = 20000    # Max number of vocab words considered\nmax_len = 80       # Max length of the review taken\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_feature)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n17465344/17464789 [==============================] - 0s 0us/step\n"
                }
            ], 
            "execution_count": 3
        }, 
        {
            "source": "## Now we pad our sequences to get equal length of each sentence input (Tx).\nx_train = sequence.pad_sequences(x_train, maxlen = max_len)\nx_test = sequence.pad_sequences(x_test, maxlen = max_len)\n\nmodel = Sequential()\nmodel.add(Embedding(max_feature, 128))\nmodel.add(LSTM(128, dropout = 0.2, recurrent_dropout =0.2))\nmodel.add(Dense(1, activation = 'sigmoid'))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 4
        }, 
        {
            "source": "from keras import backend as K\n\nK.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 5
        }, 
        {
            "source": "## Run and evaluate the model\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])\nmodel.fit(x_train, y_train, batch_size = 32, epochs = 10, validation_data = (x_test, y_test))\nmodel.evaluate(x_test, y_test, batch_size = 32)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Train on 25000 samples, validate on 25000 samples\nEpoch 1/10\n25000/25000 [==============================] - 324s 13ms/step - loss: 0.6931 - acc: 0.5042 - val_loss: 0.6929 - val_acc: 0.5002\nEpoch 2/10\n25000/25000 [==============================] - 320s 13ms/step - loss: 0.6928 - acc: 0.5130 - val_loss: 0.6926 - val_acc: 0.5274\nEpoch 3/10\n25000/25000 [==============================] - 320s 13ms/step - loss: 0.6925 - acc: 0.5183 - val_loss: 0.6924 - val_acc: 0.5135\nEpoch 4/10\n25000/25000 [==============================] - 318s 13ms/step - loss: 0.6922 - acc: 0.5286 - val_loss: 0.6921 - val_acc: 0.5205\nEpoch 5/10\n25000/25000 [==============================] - 322s 13ms/step - loss: 0.6919 - acc: 0.5424 - val_loss: 0.6918 - val_acc: 0.5590\nEpoch 6/10\n25000/25000 [==============================] - 318s 13ms/step - loss: 0.6917 - acc: 0.5482 - val_loss: 0.6916 - val_acc: 0.5230\nEpoch 7/10\n25000/25000 [==============================] - 319s 13ms/step - loss: 0.6913 - acc: 0.5510 - val_loss: 0.6912 - val_acc: 0.5278\nEpoch 8/10\n25000/25000 [==============================] - 320s 13ms/step - loss: 0.6908 - acc: 0.5488 - val_loss: 0.6907 - val_acc: 0.5765\nEpoch 9/10\n25000/25000 [==============================] - 319s 13ms/step - loss: 0.6903 - acc: 0.5633 - val_loss: 0.6902 - val_acc: 0.5712\nEpoch 10/10\n25000/25000 [==============================] - 320s 13ms/step - loss: 0.6897 - acc: 0.5733 - val_loss: 0.6897 - val_acc: 0.5586\n25000/25000 [==============================] - 59s 2ms/step\n"
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[0.68965756261825562, 0.55855999999999995]"
                    }, 
                    "execution_count": 6, 
                    "metadata": {}
                }
            ], 
            "execution_count": 6
        }, 
        {
            "source": "## Reuters Newswire Data\n\nThis dataset consists of 11,228 newswires from the Reuters news agency. Each wire is encoded as a sequence of word indexes, just as in the IMDB data. Moreover, each wire is categorised into one of 46 topics, which will serve as our label. This dataset is available through the Keras API. We will create a Multi-layer perceptron (MLP) using Keras which we can train to classify news items into the specified 46 topics.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import pip\ntry:\n    __import__('h5py')\nexcept ImportError:\n    pip.main(['install', 'h5py']) \n\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.utils import to_categorical\n\nseed = 1337\nnp.random.seed(seed)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 9
        }, 
        {
            "source": "from keras.datasets import reuters\n\nmax_words = 1000\n(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n                                                         test_split=0.2,\n                                                         seed=seed)\nnum_classes = np.max(y_train) + 1  # 46 topics", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n2113536/2110848 [==============================] - 0s 0us/step\n"
                }
            ], 
            "execution_count": 10
        }, 
        {
            "source": "Note that we cap the maximum number of words in a news item to 1000 by specifying the *num_words* key word. Also, 20% of the data will be test data and we ensure reproducibility by setting our random seed.\n\nOur training features are still simply sequences of indexes and we need to further preprocess them, so that we can plug them into a *Dense* layer. For this we use a *Tokenizer* from Keras' text preprocessing module. This tokenizer will take an index sequence and map it to a vector of length *max_words=1000*. Each of the 1000 vector positions corresponds to one of the words in our newswire corpus. The output of the tokenizer has a 1 at the i-th position of the vector, if the word corresponding to i is in the description of the newswire, and 0 otherwise. Even if this word appears multiple times, we still just put a 1 into our vector, i.e. our tokenizer is binary. We use this tokenizer to transform both train and test features:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words=max_words)\nx_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\nx_test = tokenizer.sequences_to_matrix(x_test, mode='binary')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 11
        }, 
        {
            "source": "y_train = to_categorical(y_train, num_classes)\ny_test = to_categorical(y_test, num_classes)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 12
        }, 
        {
            "source": "model = Sequential()  \nmodel.add(Dense(512, activation='relu', input_shape = (max_words,))) \nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax')) \n\nmodel.compile(loss = 'categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 13
        }, 
        {
            "source": "# Keras takes heavy resource consumption, this is the workaround to limit resource consumption on (IBM) cloud\nfrom keras import backend as K\n\nK.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 14
        }, 
        {
            "source": "batch_size = 32\nmodel.fit(x_train, y_train, batch_size = batch_size, epochs=5)\nscore = model.evaluate(x_test, y_test)\nmodel.save(\"model.h5\")", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Epoch 1/5\n8982/8982 [==============================] - 4s 423us/step - loss: 1.3914 - acc: 0.6925\nEpoch 2/5\n8982/8982 [==============================] - 3s 383us/step - loss: 0.7703 - acc: 0.8195\nEpoch 3/5\n8982/8982 [==============================] - 3s 382us/step - loss: 0.5552 - acc: 0.8661\nEpoch 4/5\n8982/8982 [==============================] - 4s 409us/step - loss: 0.4219 - acc: 0.8936\nEpoch 5/5\n8982/8982 [==============================] - 4s 392us/step - loss: 0.3499 - acc: 0.9093\n2246/2246 [==============================] - 0s 138us/step\n"
                }
            ], 
            "execution_count": 15
        }, 
        {
            "source": "print(\"accuracy of the model is {:2f'}%\".format(score[1]))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}